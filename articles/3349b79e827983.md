---
title: "Sudachipyを使って方言を標準語に変換するコードが書けた" # 記事のタイトル
emoji: "😶" # アイキャッチとして使われる絵文字（1文字だけ）
type: "tech" # tech: 技術記事 / idea: アイデア記事
topics: ["Python","NLP","形態素解析","Sudachi","SudachiPy"] # タグ。["markdown", "rust", "aws"]のように指定する
published: true # 公開設定（falseにすると下書き）
---

```
注意：こちらの記事は、Sudachipy v0.6.3 環境下においてです。
```

# やりたかったこと
1.ユーザー辞書に方言を登録
2.入力文を分かち書きして方言ならば正規化表現機能で変換
3.がっちゃんこして出力
4.(おまけ)それを改めてsudachi_dict fullにて分かち書き

# 私について
pythonを触り始めて3ヶ月のペーペーです。

# 実際のコード

```
# import sudachipy
from sudachipy import tokenizer
from sudachipy import dictionary

# トークナイザの作成,辞書位置(sudachi.json ; 相対パス)の指定
# Qsudachidict_full優先 dict="full"
# mode(a,b,c)の指定
config_path_link = "lib/python3.9/site-packages/sudachipy/resources/sudachi.json"
tokenizer_obj = dictionary.Dictionary(config_path=config_path_link).create() 
mode = tokenizer.Tokenizer.SplitMode.C

# 方言辞書(ユーザー辞書)とシステム辞書を元にわかち書きを行い、方言->標準語へ置き換え後接続する関数
# output type(list)
# [0]->それぞれのわかち書きに対して、方言or Not表記
# [1]->標準語変換,結合後の文章
def transHogenToJp(text,mode,tokenizer_obj):
    combinedExchangeHogen = []
    hogenOrNotArray = []
    tokens = tokenizer_obj.tokenize(text,mode)
    for m in tokens:
        if m.part_of_speech()[5] == '方言':
            hogenOrNotArray.append(m.surface()+':方言->'+m.normalized_form()+':標準語')
            combinedExchangeHogen.append(m.normalized_form())
        else:
            hogenOrNotArray.append(m.surface()+':標準語')
            combinedExchangeHogen.append(m.surface())
    return combinedExchangeHogen,hogenOrNotArray

#m.part_of_speech()[5]-> ユーザー辞書を作成するときに、一番右を'方言'と登録したのでこうしました。
#m.part_of_speech() # => ['動詞', '一般', '*', '*', '下一段-バ行', '連用形-一般']

text = input()
print(transHogenToJp(text,mode,tokenizer_obj)[0])
print(transHogenToJp(text,mode,tokenizer_obj)[1])

# list -> str
text = "".join(transHogenToJp(text,mode,tokenizer_obj)[0])

tokenizer_obj = dictionary.Dictionary(dict="full").create() #Qsudachidict_full優先 dict="full"
mode = tokenizer.Tokenizer.SplitMode.C

# 標準語を入力し単純にわかち書きして出力する関数
def wakachiWrite(text,mode,tokenizer_obj):
    tokens = tokenizer_obj.tokenize(text,mode)
    wakachi_Array = []
    for m in tokens:
        wakachi_Array.append(m.surface())
    return wakachi_Array

# output type;list
print(wakachiWrite(text,mode,tokenizer_obj))
# list -> strs
print("".join(wakachiWrite(text,mode,tokenizer_obj)))

#### Sudachipy Memo ####
#m.surface() # => '食べ'
#m.dictionary_form() # => '食べる'
#m.reading_form() # => 'タベ'
#m.part_of_speech() # => ['動詞', '一般', '*', '*', '下一段-バ行', '連用形-一般']
#m.normalized_form() => 正規化
```
# 今後の課題
工藤著にもある通りですが、結局ユーザー辞書の拡充をすることが未知語処理の精度向上に役に立つそうですね。これからは、辞書の拡充と、精度判定のためのinput文章の収集を頑張ります。

また、長野高専の長野弁の論文では、同音意義方言があるときに処理が必要みたいですが、それは方言によって様々でしょう。
私は現在ずーずー弁を扱っているので、辞書の拡充以外の方法で低コストなずーずー弁の例外処理・前処理が発見できたら嬉しいって感じです。

それと同時に、cpaCy/Ginzaを使って構文解析,意味解析,文脈解析をいじってみようかなと思っています。
できるかなぁ

最後まで読んでいただきありがとうございました。

# 参考文献
https://github.com/WorksApplications/SudachiPy/blob/develop/docs/tutorial.md
https://evrythingonmac.blogspot.com/2019/12/sudachipy-sudachidictfull-update.html
https://ohke.hateblo.jp/entry/2019/03/09/101500
https://www.jstage.jst.go.jp/article/pjsai/JSAI2009/0/JSAI2009_3I13/_pdf/-char/ja
https://amzn.to/3pEbooo